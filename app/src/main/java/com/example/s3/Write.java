/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.example.s3;

import org.apache.avro.Schema;
import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.*;
import org.apache.iceberg.avro.AvroSchemaUtil;
import org.apache.iceberg.aws.s3.S3FileIO;
import org.apache.iceberg.aws.s3.S3FileIOProperties;
import org.apache.iceberg.catalog.Catalog;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.data.GenericRecord;
import org.apache.iceberg.data.Record;
import org.apache.iceberg.data.parquet.GenericParquetWriter;
import org.apache.iceberg.io.FileAppender;
import org.apache.iceberg.io.OutputFile;
import org.apache.iceberg.jdbc.JdbcCatalog;
import org.apache.iceberg.parquet.Parquet;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 *  Data file : S3.
 *  Metadata tables: Jdbc catalog (sqlite db)
 *  Metadata files: S3
 *
 * Steps:
 * 1. Run docker-compose.yml
 * 2. Create required buckets
 * $ aws --endpoint-url http://localhost:4566 --region us-west-2 s3api create-bucket --bucket iceberg-warehouse --create-bucket-configuration LocationConstraint=us-west-2
 * Result :
 * {
 *     "Location": "http://iceberg-warehouse.s3.localhost.localstack.cloud:4566/"
 * }
 * $ aws --endpoint-url http://localhost:4566 --region us-west-2 s3api create-bucket --bucket application-data --create-bucket-configuration LocationConstraint=us-west-2
 * Result:
 * {
 *     "Location": "http://application-data.s3.localhost.localstack.cloud:4566/"
 * }
 * 3. Run main
 *
 *
 * 4. Results:
 * $ aws --endpoint-url=http://localhost:4566 s3 ls s3://application-data/ --recursive
 * 2024-09-12 15:11:17        697 jdbc-table/data-file.parquet
 *
 * $ aws --endpoint-url=http://localhost:4566 s3 ls s3://iceberg-warehouse/ --recursive
 * 2024-09-12 15:11:16       1011 jdbc_s3_namespace/jdbc_table/metadata/00000-bfda89b7-26a0-45f8-97ce-c8e67d7873b9.metadata.json
 * 2024-09-12 15:11:18       2162 jdbc_s3_namespace/jdbc_table/metadata/00001-db114320-674e-476a-906e-8e16daf9170a.metadata.json
 * 2024-09-12 15:11:18       6586 jdbc_s3_namespace/jdbc_table/metadata/11c29ccc-fdfe-4ff8-a542-8d82f6730ec9-m0.avro
 * 2024-09-12 15:11:18       4456 jdbc_s3_namespace/jdbc_table/metadata/snap-1310763839404423519-1-11c29ccc-fdfe-4ff8-a542-8d82f6730ec9.avro
 *
 *
 * $ duckdb catalogs-s3.db
 * v0.9.2 3c695d7ba9
 * Enter ".help" for usage hints.
 * D show tables;
 * ┌──────────────────────────────┐
 * │             name             │
 * │           varchar            │
 * ├──────────────────────────────┤
 * │ iceberg_namespace_properties │
 * │ iceberg_tables               │
 * └──────────────────────────────┘
 * D select * from iceberg_namespace_properties;
 * ┌──────────────┬───────────┬──────────────┬────────────────┐
 * │ catalog_name │ namespace │ property_key │ property_value │
 * │   varchar    │  varchar  │   varchar    │    varchar     │
 * ├──────────────────────────────────────────────────────────┤
 * │                          0 rows                          │
 * └──────────────────────────────────────────────────────────┘
 * D select * from iceberg_tables;
 * ┌───────────────────┬───────────────────┬────────────┬──────────────────────┬──────────────────────────────────────────────────────────────────────┐
 * │   catalog_name    │  table_namespace  │ table_name │  metadata_location   │                      previous_metadata_location                      │
 * │      varchar      │      varchar      │  varchar   │       varchar        │                               varchar                                │
 * ├───────────────────┼───────────────────┼────────────┼──────────────────────┼──────────────────────────────────────────────────────────────────────┤
 * │ test_rest_catalog │ jdbc_s3_namespace │ jdbc_table │ s3://iceberg-wareh…  │ s3://iceberg-warehouse/jdbc_s3_namespace/jdbc_table/metadata/00000…  │
 * └───────────────────┴───────────────────┴────────────┴──────────────────────┴──────────────────────────────────────────────────────────────────────┘
 * D
 */

public class Write {
    public static void main(String[] args) throws IOException {
        // Define the Avro schema for the Parquet file
        String schema = "{\"type\": \"record\", \"name\": \"TestRecord\", \"fields\": ["
                + "{\"name\": \"id\", \"type\": \"int\"},"
                + "{\"name\": \"name\", \"type\": \"string\"}"
                + "]}";

        Schema avroSchema = new Schema.Parser().parse(schema);
        org.apache.iceberg.Schema icebergSchema = AvroSchemaUtil.toIceberg(avroSchema);

        Map<String, String> properties = new HashMap<>();
        properties.put(CatalogProperties.CATALOG_IMPL, JdbcCatalog.class.getName());
        properties.put(CatalogProperties.URI, "jdbc:sqlite:file:/Users/ADASARI/work/learning/iceberg/catalog/catalogs-s3.db");
        properties.put(CatalogProperties.WAREHOUSE_LOCATION, "s3://iceberg-warehouse"); // add access key details. optional for localstack.
        properties.put("s3.endpoint", "http://localhost:4566");
        properties.put("s3.path-style-access", "true");
        properties.put("io-impl", "org.apache.iceberg.aws.s3.S3FileIO");

        Configuration conf = new Configuration();
        Catalog catalog = CatalogUtil.buildIcebergCatalog("test_rest_catalog", properties, conf);

        Map<String, String> s3Properties = new HashMap<>();
        s3Properties.put(S3FileIOProperties.ENDPOINT, "http://localhost:4566");  // S3 endpoint
//        s3Properties.put(S3FileIOProperties.ACCESS_KEY_ID, "admin"); // unused in localstack env.
//        s3Properties.put(S3FileIOProperties.SECRET_ACCESS_KEY, "password");
        s3Properties.put(S3FileIOProperties.PATH_STYLE_ACCESS, "true");

        // S3FileIO to write data files to s3.
        S3FileIO s3FileIO = new S3FileIO();
        s3FileIO.initialize(s3Properties);

        // Get table from Catalog
        TableIdentifier tableIdentifier = TableIdentifier.of("jdbc_s3_namespace", "jdbc_table");
        if (catalog.tableExists(tableIdentifier)) {
            System.out.println("Table already exists.");
        } else {
            catalog.createTable(tableIdentifier, icebergSchema, PartitionSpec.unpartitioned());
        }

        Table table = catalog.loadTable(tableIdentifier);

        // Create a record
        Record record = GenericRecord.create(icebergSchema);
        record.setField("id", 1);
        record.setField("name", "Iceberg Test");

        OutputFile outputFile = s3FileIO.newOutputFile("s3://application-data/jdbc-table/data-file.parquet");

        FileAppender<Record> appender = null;
        try {
            appender = Parquet.write(outputFile)
                    .schema(table.schema())
                    .createWriterFunc(GenericParquetWriter::buildWriter)
                    .build();

            // Write the record to the Parquet file
            appender.add(record);
        } finally {
            if (appender != null) {
                appender.close();
            }
        }

        DataFile dataFile = DataFiles.builder(PartitionSpec.unpartitioned())
                .withPath(outputFile.location())
                .withFileSizeInBytes(appender.length()) // Set the file size
                .withRecordCount(1) // Set record count
                .build();

        table.newAppend()
                .appendFile(dataFile) // Commit the file
                .commit();
    }
}
